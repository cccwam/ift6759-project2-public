{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "networks_seq2seq_nmt.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "source": [],
        "metadata": {
          "collapsed": false
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bl9GdT7h0Hxk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhwgQAn50EZp",
        "colab_type": "text"
      },
      "source": [
        "# TensorFlow Addons Networks : Sequence-to-Sequence NMT with Attention Mechanism\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/addons/tutorials/networks_seq2seq_nmt\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/addons/blob/master/docs/tutorials/networks_seq2seq_nmt.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/addons/blob/master/docs/tutorials/networks_seq2seq_nmt.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "      <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/addons/docs/tutorials/networks_seq2seq_nmt.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ip0n8178Fuwm",
        "colab_type": "text"
      },
      "source": [
        "## Overview\n",
        "This notebook gives a brief introduction into the ***Sequence to Sequence Model Architecture***\n",
        "In this noteboook we broadly cover four essential topics necessary for Neural Machine Translation:\n",
        "\n",
        "\n",
        "* **Data cleaning**\n",
        "* **Data preparation**\n",
        "* **Neural Translation Model with Attention**\n",
        "* **Final Translation**\n",
        "\n",
        "The basic idea behind such a model though, is only the encoder-decoder architecture. These networks are usually used for a variety of tasks like text-summerization, Machine translation, Image Captioning, etc. This tutorial provideas a hands-on understanding of the concept, explaining the technical jargons wherever necessary. We focus on the task of Neural Machine Translation (NMT) which was the very first testbed for seq2seq models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNiadLKNLleD",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bUHYPhlF-Ql",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1ZvFhKf9DTC",
        "colab_type": "code",
        "outputId": "e671c904-59f8-4c35-9d79-88a2b8a9767c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51c5f88z9VZR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append(\"..\") # Require to have the utilities packages in path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jw044zGCZp-K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except:\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82GcQTsGf414",
        "colab_type": "text"
      },
      "source": [
        "## Additional Resources:\n",
        "\n",
        "These are a lst of resurces you must install in order to allow you to run this notebook:\n",
        "\n",
        "\n",
        "1. [German-English Dataset](http://www.manythings.org/anki/deu-eng.zip)\n",
        "\n",
        "\n",
        "The dataset should be downloaded, in order to compile this notebook, the embeddings can be used, as they are pretrained. Though, we carry out our own training here !!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "co6-YpBwL-4d",
        "colab_type": "code",
        "outputId": "8379028e-eb02-4035-d0ca-0f834bf29dae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from pathlib import Path\n",
        "import csv\n",
        "import string\n",
        "import re\n",
        "import pandas as pd\n",
        "from pickle import dump\n",
        "from unicodedata import normalize\n",
        "from numpy import array\n",
        "import itertools\n",
        "from pickle import load\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM\n",
        "import tensorflow.keras.layers\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from pickle import load\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow_addons as tfa\n",
        "import unicodedata\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCg1VaQR9Mpt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_path = Path(r\"/content/drive/My Drive/Colab Notebooks/IFT 6759/Project 2/Data/\")\n",
        "files = list(data_path.glob(\"*\"))\n",
        "\n",
        "with open(data_path / \"unaligned.en\", 'r') as f:\n",
        "    unaligned_en = [line.rstrip() for line in f] # Remove the \\n\n",
        "    unaligned_en = pd.DataFrame(unaligned_en, columns=[\"text\"])\n",
        "    f.close()\n",
        "    \n",
        "with open(data_path / \"unaligned.fr\", 'r') as f:\n",
        "    unaligned_fr = [line.rstrip() for line in f] # Remove the \\n\n",
        "    unaligned_fr = pd.DataFrame(unaligned_fr, columns=[\"text\"])\n",
        "    f.close()\n",
        "\n",
        "with open(data_path / \"train.lang1\", 'r') as f:\n",
        "    train_lang1_en = [line.rstrip() for line in f] # Remove the \\n\n",
        "    train_lang1_en = pd.DataFrame(train_lang1_en, columns=[\"text\"])\n",
        "    f.close()\n",
        "\n",
        "with open(data_path / \"train.lang2\", 'r') as f:\n",
        "    train_lang2_fr = [line.rstrip() for line in f] # Remove the \\n\n",
        "    train_lang2_fr = pd.DataFrame(train_lang2_fr, columns=[\"text\"])\n",
        "    f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7gjUT_9XSoj",
        "colab_type": "text"
      },
      "source": [
        "## Data Cleaning\n",
        "\n",
        "Our data set is a German-English translation dataset. It contains 152,820 pairs of English to German phases, one pair per line with a tab separating the language. These dataset though organized needs cleaning before we can work on it. This will enable us to remove unnecessary bumps that may come in during the training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXpft1qQknO8",
        "colab_type": "text"
      },
      "source": [
        "## Saving the Cleaned Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VuAlI1m-BoR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_dataset():\n",
        "    frText = []\n",
        "    enText = []\n",
        "    for ((indexFr, rowFr), (indexEn, rowEn))  in zip(train_lang2_fr.iterrows(), train_lang1_en.iterrows()):\n",
        "        frText.append(preprocess_sentence(rowFr['text']))\n",
        "        enText.append(preprocess_sentence(rowEn['text']))\n",
        "\n",
        "    return frText, enText\n",
        "\n",
        "def unicode_to_ascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "    w = unicode_to_ascii(w.lower().strip())\n",
        "\n",
        "    # creating a space between a word and the punctuation following it\n",
        "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "\n",
        "    w = w.rstrip().strip()\n",
        "\n",
        "    # adding a start and an end token to the sentence\n",
        "    # so that the model know when to start and stop predicting.\n",
        "    w = '<start> ' + w + ' <end>'\n",
        "    return w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p22ejycS-V57",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fr, en = create_dataset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfzJw-qt-zyu",
        "colab_type": "code",
        "outputId": "ada3aab4-dc04-489a-abaa-13ce457abde1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "frTrain, frValid, enTrain, enValid  = train_test_split(fr,en,test_size=0.2, shuffle = False)\n",
        "print(enTrain[0], '\\n', frTrain[0], '\\n', enValid[0],  '\\n', frValid[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> so too does the idea that accommodating religious differences is dangerous <end> \n",
            " <start> l idee de concilier les differences religieuses semble donc dangereuse . <end> \n",
            " <start> my wish is for you to help a strong sustainable movement to educate every child about food to inspire families to cook again and to empower people everywhere to fight obesity <end> \n",
            " <start> mon souhait est que vous souteniez un puissant mouvement durable pour eduquer chaque enfant a l alimentation , pour inspirer les familles a cuisiner a nouveau , et dynamiser les gens partout a lutter contre l obesite . <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cfb66QxWYr6A",
        "colab_type": "text"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oq60MBPSanQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# English tokenizer\n",
        "en_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', oov_token=\"<unk>\")\n",
        "en_tokenizer.fit_on_texts(enTrain)\n",
        "# English train\n",
        "data_en_train = en_tokenizer.texts_to_sequences(enTrain)\n",
        "data_en_train = tf.keras.preprocessing.sequence.pad_sequences(data_en_train,padding='post')\n",
        "# English valid\n",
        "data_en_valid = en_tokenizer.texts_to_sequences(enValid)\n",
        "data_en_valid = tf.keras.preprocessing.sequence.pad_sequences(data_en_valid,padding='post')\n",
        "\n",
        "# French tokenizer\n",
        "fr_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', oov_token=\"<unk>\")\n",
        "fr_tokenizer.fit_on_texts(frTrain)\n",
        "# French train\n",
        "data_fr_train = fr_tokenizer.texts_to_sequences(frTrain)\n",
        "data_fr_train = tf.keras.preprocessing.sequence.pad_sequences(data_fr_train,padding='post')\n",
        "# French valid\n",
        "data_fr_valid = fr_tokenizer.texts_to_sequences(frValid)\n",
        "data_fr_valid = tf.keras.preprocessing.sequence.pad_sequences(data_fr_valid,padding='post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH5oSRNeSc1s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def max_len(tensor):\n",
        "    #print( np.argmax([len(t) for t in tensor]))\n",
        "    return max( len(t) for t in tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdM37lNBGXAj",
        "colab_type": "text"
      },
      "source": [
        "## Model Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfiBUJM2Et6C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = data_en_train\n",
        "Y_train = data_fr_train\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = len(X_train)\n",
        "steps_per_epoch = BUFFER_SIZE//BATCH_SIZE\n",
        "embedding_dims = 256\n",
        "rnn_units = 1024\n",
        "dense_units = 1024\n",
        "Dtype = tf.float32   #used to initialize DecoderCell Zero state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ff_jQHLhGqJU",
        "colab_type": "text"
      },
      "source": [
        "## Dataset Prepration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b__1hPHVFALO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Tx = max_len(data_en_train)\n",
        "Ty = max_len(data_fr_train)  \n",
        "\n",
        "input_vocab_size = len(en_tokenizer.word_index)+1  \n",
        "output_vocab_size = len(fr_tokenizer.word_index)+ 1\n",
        "dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "example_X, example_Y = next(iter(dataset))\n",
        "#print(example_X.shape) \n",
        "#print(example_Y.shape) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQRgJcYgapqE",
        "colab_type": "text"
      },
      "source": [
        "## Defining NMT Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGdakRtjaokF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ENCODER\n",
        "class EncoderNetwork(tf.keras.Model):\n",
        "    def __init__(self,input_vocab_size,embedding_dims, rnn_units ):\n",
        "        super().__init__()\n",
        "        self.encoder_embedding = tf.keras.layers.Embedding(input_dim=input_vocab_size,\n",
        "                                                           output_dim=embedding_dims)\n",
        "        self.encoder_rnnlayer = tf.keras.layers.LSTM(rnn_units,return_sequences=True, \n",
        "                                                     return_state=True )\n",
        "\n",
        "    \n",
        "#DECODER\n",
        "class DecoderNetwork(tf.keras.Model):\n",
        "    def __init__(self,output_vocab_size, embedding_dims, rnn_units):\n",
        "        super().__init__()\n",
        "        self.decoder_embedding = tf.keras.layers.Embedding(input_dim=output_vocab_size,\n",
        "                                                           output_dim=embedding_dims) \n",
        "        self.dense_layer = tf.keras.layers.Dense(output_vocab_size)\n",
        "        self.decoder_rnncell = tf.keras.layers.LSTMCell(rnn_units)\n",
        "        # Sampler\n",
        "        self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
        "        # Create attention mechanism with memory = None\n",
        "        self.attention_mechanism = self.build_attention_mechanism(dense_units,None,BATCH_SIZE*[Tx])\n",
        "        self.rnn_cell =  self.build_rnn_cell(BATCH_SIZE)\n",
        "        self.decoder = tfa.seq2seq.BasicDecoder(self.rnn_cell, sampler= self.sampler,\n",
        "                                                output_layer=self.dense_layer)\n",
        "\n",
        "    def build_attention_mechanism(self, units,memory, memory_sequence_length):\n",
        "        return tfa.seq2seq.LuongAttention(units, memory = memory, \n",
        "                                          memory_sequence_length=memory_sequence_length)\n",
        "        #return tfa.seq2seq.BahdanauAttention(units, memory = memory, memory_sequence_length=memory_sequence_length)\n",
        "\n",
        "    # wrap decodernn cell  \n",
        "    def build_rnn_cell(self, batch_size ):\n",
        "        rnn_cell = tfa.seq2seq.AttentionWrapper(self.decoder_rnncell, self.attention_mechanism,\n",
        "                                                attention_layer_size=dense_units)\n",
        "        return rnn_cell\n",
        "    \n",
        "    def build_decoder_initial_state(self, batch_size, encoder_state,Dtype):\n",
        "        decoder_initial_state = self.rnn_cell.get_initial_state(batch_size = batch_size, \n",
        "                                                                dtype = Dtype)\n",
        "        decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state) \n",
        "        return decoder_initial_state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPwcfddTa0oB",
        "colab_type": "text"
      },
      "source": [
        "## Initializing Training functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1BEqVyra2jW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_function(y_pred, y):\n",
        "   \n",
        "    #shape of y [batch_size, ty]\n",
        "    #shape of y_pred [batch_size, Ty, output_vocab_size] \n",
        "    sparsecategoricalcrossentropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
        "                                                                                  reduction='none')\n",
        "    loss = sparsecategoricalcrossentropy(y_true=y, y_pred=y_pred)\n",
        "    mask = tf.logical_not(tf.math.equal(y,0))   #output 0 for y=0 else output 1\n",
        "    mask = tf.cast(mask, dtype=loss.dtype)\n",
        "    loss = mask* loss\n",
        "    loss = tf.reduce_mean(loss)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def train_step(input_batch, output_batch,encoder_initial_cell_state):\n",
        "    #initialize loss = 0\n",
        "    loss = 0\n",
        "    with tf.GradientTape() as tape:\n",
        "        encoder_emb_inp = encoderNetwork.encoder_embedding(input_batch)\n",
        "        a, a_tx, c_tx = encoderNetwork.encoder_rnnlayer(encoder_emb_inp, \n",
        "                                                        initial_state =encoder_initial_cell_state)\n",
        "\n",
        "        #[last step activations,last memory_state] of encoder passed as input to decoder Network\n",
        "        \n",
        "         \n",
        "        # Prepare correct Decoder input & output sequence data\n",
        "        decoder_input = output_batch[:,:-1] # ignore <end>\n",
        "        #compare logits with timestepped +1 version of decoder_input\n",
        "        decoder_output = output_batch[:,1:] #ignore <start>\n",
        "\n",
        "\n",
        "        # Decoder Embeddings\n",
        "        decoder_emb_inp = decoderNetwork.decoder_embedding(decoder_input)\n",
        "\n",
        "        #Setting up decoder memory from encoder output and Zero State for AttentionWrapperState\n",
        "        decoderNetwork.attention_mechanism.setup_memory(a)\n",
        "        decoder_initial_state = decoderNetwork.build_decoder_initial_state(BATCH_SIZE,\n",
        "                                                                           encoder_state=[a_tx, c_tx],\n",
        "                                                                           Dtype=tf.float32)\n",
        "        \n",
        "        #BasicDecoderOutput        \n",
        "        outputs, _, _ = decoderNetwork.decoder(decoder_emb_inp,initial_state=decoder_initial_state,\n",
        "                                               sequence_length=BATCH_SIZE*[Ty-1])\n",
        "\n",
        "        logits = outputs.rnn_output\n",
        "        #Calculate loss\n",
        "\n",
        "        loss = loss_function(logits, decoder_output)\n",
        "\n",
        "    #Returns the list of all layer variables / weights.\n",
        "    variables = encoderNetwork.trainable_variables + decoderNetwork.trainable_variables  \n",
        "    # differentiate loss wrt variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "\n",
        "    #grads_and_vars – List of(gradient, variable) pairs.\n",
        "    grads_and_vars = zip(gradients,variables)\n",
        "    optimizer.apply_gradients(grads_and_vars)\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71Lkdx6GFb3A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#RNN LSTM hidden and memory state initializer\n",
        "def initialize_initial_state():\n",
        "        return [tf.zeros((BATCH_SIZE, rnn_units)), tf.zeros((BATCH_SIZE, rnn_units))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5uzLcu2bNX3",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvfD2SknWrt6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(epochs): \n",
        "  for i in range(1, epochs+1):\n",
        "\n",
        "      encoder_initial_cell_state = initialize_initial_state()\n",
        "      total_loss = 0.0\n",
        "\n",
        "      for ( batch , (input_batch, output_batch)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "          batch_loss = train_step(input_batch, output_batch, encoder_initial_cell_state)\n",
        "          total_loss += batch_loss\n",
        "          if (batch+1)%5 == 0:\n",
        "              print(\"total loss: {} epoch {} batch {} \".format(batch_loss.numpy(), i, batch+1))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbjn4DRNRfbQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoderNetwork = EncoderNetwork(input_vocab_size,embedding_dims, rnn_units)\n",
        "decoderNetwork = DecoderNetwork(output_vocab_size,embedding_dims, rnn_units)\n",
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Paig46wVSEAW",
        "colab_type": "code",
        "outputId": "0d91a2c7-7eae-4e75-95ba-58ea4b2f7a4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train(15)\n",
        "\n",
        "#encoderNetwork.save_weights()\n",
        "#torch.save(decoderNetwork.state_dict(), 'decoder.dict')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total loss: 1.9634898900985718 epoch 1 batch 5 \n",
            "total loss: 1.7855565547943115 epoch 1 batch 10 \n",
            "total loss: 1.4182482957839966 epoch 1 batch 15 \n",
            "total loss: 1.4669640064239502 epoch 1 batch 20 \n",
            "total loss: 1.5078994035720825 epoch 1 batch 25 \n",
            "total loss: 1.5681970119476318 epoch 1 batch 30 \n",
            "total loss: 1.3845863342285156 epoch 1 batch 35 \n",
            "total loss: 1.5007833242416382 epoch 1 batch 40 \n",
            "total loss: 1.3609955310821533 epoch 1 batch 50 \n",
            "total loss: 1.5118592977523804 epoch 1 batch 55 \n",
            "total loss: 1.610312581062317 epoch 1 batch 60 \n",
            "total loss: 1.4810110330581665 epoch 1 batch 65 \n",
            "total loss: 1.3774493932724 epoch 1 batch 70 \n",
            "total loss: 1.4326605796813965 epoch 1 batch 75 \n",
            "total loss: 1.3058146238327026 epoch 1 batch 80 \n",
            "total loss: 1.2256760597229004 epoch 1 batch 85 \n",
            "total loss: 1.2180213928222656 epoch 1 batch 90 \n",
            "total loss: 1.36214280128479 epoch 1 batch 95 \n",
            "total loss: 1.245031476020813 epoch 1 batch 100 \n",
            "total loss: 1.4819265604019165 epoch 1 batch 105 \n",
            "total loss: 1.4781012535095215 epoch 1 batch 110 \n",
            "total loss: 1.3265631198883057 epoch 1 batch 115 \n",
            "total loss: 1.2294930219650269 epoch 1 batch 120 \n",
            "total loss: 1.2705388069152832 epoch 1 batch 125 \n",
            "total loss: 1.1836450099945068 epoch 1 batch 130 \n",
            "total loss: 1.2410310506820679 epoch 1 batch 135 \n",
            "total loss: 1.2739641666412354 epoch 2 batch 5 \n",
            "total loss: 1.258036494255066 epoch 2 batch 10 \n",
            "total loss: 1.2637704610824585 epoch 2 batch 15 \n",
            "total loss: 1.1807602643966675 epoch 2 batch 20 \n",
            "total loss: 1.2751715183258057 epoch 2 batch 25 \n",
            "total loss: 1.202805757522583 epoch 2 batch 30 \n",
            "total loss: 1.2298659086227417 epoch 2 batch 35 \n",
            "total loss: 1.3035647869110107 epoch 2 batch 40 \n",
            "total loss: 1.4517087936401367 epoch 2 batch 45 \n",
            "total loss: 1.180711269378662 epoch 2 batch 50 \n",
            "total loss: 1.1076056957244873 epoch 2 batch 55 \n",
            "total loss: 1.322643518447876 epoch 2 batch 60 \n",
            "total loss: 1.2553547620773315 epoch 2 batch 65 \n",
            "total loss: 1.2711009979248047 epoch 2 batch 70 \n",
            "total loss: 1.1754114627838135 epoch 2 batch 75 \n",
            "total loss: 1.2921640872955322 epoch 2 batch 80 \n",
            "total loss: 1.2863093614578247 epoch 2 batch 85 \n",
            "total loss: 1.3279343843460083 epoch 2 batch 90 \n",
            "total loss: 1.2077654600143433 epoch 2 batch 95 \n",
            "total loss: 1.3711163997650146 epoch 2 batch 100 \n",
            "total loss: 1.2724047899246216 epoch 2 batch 105 \n",
            "total loss: 1.136523723602295 epoch 2 batch 110 \n",
            "total loss: 1.3002517223358154 epoch 2 batch 115 \n",
            "total loss: 1.2062519788742065 epoch 2 batch 120 \n",
            "total loss: 1.1296401023864746 epoch 2 batch 125 \n",
            "total loss: 1.1387341022491455 epoch 2 batch 130 \n",
            "total loss: 1.3083178997039795 epoch 2 batch 135 \n",
            "total loss: 1.0559762716293335 epoch 3 batch 5 \n",
            "total loss: 1.1891282796859741 epoch 3 batch 10 \n",
            "total loss: 1.0354784727096558 epoch 3 batch 15 \n",
            "total loss: 1.167822241783142 epoch 3 batch 20 \n",
            "total loss: 1.12919020652771 epoch 3 batch 25 \n",
            "total loss: 1.1308565139770508 epoch 3 batch 30 \n",
            "total loss: 1.2396049499511719 epoch 3 batch 35 \n",
            "total loss: 1.1688324213027954 epoch 3 batch 40 \n",
            "total loss: 1.2166507244110107 epoch 3 batch 45 \n",
            "total loss: 1.0050461292266846 epoch 3 batch 50 \n",
            "total loss: 1.2636550664901733 epoch 3 batch 55 \n",
            "total loss: 1.1455501317977905 epoch 3 batch 60 \n",
            "total loss: 1.153475284576416 epoch 3 batch 65 \n",
            "total loss: 1.1100683212280273 epoch 3 batch 70 \n",
            "total loss: 1.1460179090499878 epoch 3 batch 75 \n",
            "total loss: 1.0542134046554565 epoch 3 batch 80 \n",
            "total loss: 1.1045339107513428 epoch 3 batch 85 \n",
            "total loss: 1.0945229530334473 epoch 3 batch 90 \n",
            "total loss: 1.1866117715835571 epoch 3 batch 95 \n",
            "total loss: 1.0643689632415771 epoch 3 batch 100 \n",
            "total loss: 1.2036409378051758 epoch 3 batch 105 \n",
            "total loss: 1.1106743812561035 epoch 3 batch 110 \n",
            "total loss: 1.1408088207244873 epoch 3 batch 115 \n",
            "total loss: 1.0704351663589478 epoch 3 batch 120 \n",
            "total loss: 1.1348230838775635 epoch 3 batch 125 \n",
            "total loss: 1.0868992805480957 epoch 3 batch 130 \n",
            "total loss: 1.280826210975647 epoch 3 batch 135 \n",
            "total loss: 1.0225400924682617 epoch 4 batch 5 \n",
            "total loss: 1.0073343515396118 epoch 4 batch 10 \n",
            "total loss: 0.9513489007949829 epoch 4 batch 15 \n",
            "total loss: 1.2096072435379028 epoch 4 batch 20 \n",
            "total loss: 1.0210877656936646 epoch 4 batch 25 \n",
            "total loss: 1.0509157180786133 epoch 4 batch 30 \n",
            "total loss: 1.0751218795776367 epoch 4 batch 35 \n",
            "total loss: 1.071258306503296 epoch 4 batch 40 \n",
            "total loss: 1.1116713285446167 epoch 4 batch 45 \n",
            "total loss: 1.1054672002792358 epoch 4 batch 50 \n",
            "total loss: 1.1263275146484375 epoch 4 batch 55 \n",
            "total loss: 1.0353295803070068 epoch 4 batch 60 \n",
            "total loss: 1.2427551746368408 epoch 4 batch 65 \n",
            "total loss: 1.1389039754867554 epoch 4 batch 70 \n",
            "total loss: 0.8900383114814758 epoch 4 batch 75 \n",
            "total loss: 1.1042628288269043 epoch 4 batch 80 \n",
            "total loss: 1.1647238731384277 epoch 4 batch 85 \n",
            "total loss: 1.203963279724121 epoch 4 batch 90 \n",
            "total loss: 1.085280179977417 epoch 4 batch 95 \n",
            "total loss: 1.113318920135498 epoch 4 batch 100 \n",
            "total loss: 1.1147770881652832 epoch 4 batch 105 \n",
            "total loss: 1.187355637550354 epoch 4 batch 110 \n",
            "total loss: 0.9886627197265625 epoch 4 batch 115 \n",
            "total loss: 0.9776429533958435 epoch 4 batch 120 \n",
            "total loss: 1.0281492471694946 epoch 4 batch 125 \n",
            "total loss: 1.0113816261291504 epoch 4 batch 130 \n",
            "total loss: 1.1506119966506958 epoch 4 batch 135 \n",
            "total loss: 0.9014326333999634 epoch 5 batch 5 \n",
            "total loss: 0.940185546875 epoch 5 batch 10 \n",
            "total loss: 0.9501511454582214 epoch 5 batch 15 \n",
            "total loss: 0.9638354778289795 epoch 5 batch 20 \n",
            "total loss: 1.1495863199234009 epoch 5 batch 25 \n",
            "total loss: 1.0606200695037842 epoch 5 batch 30 \n",
            "total loss: 0.9512829184532166 epoch 5 batch 35 \n",
            "total loss: 1.0818474292755127 epoch 5 batch 40 \n",
            "total loss: 0.9883320927619934 epoch 5 batch 45 \n",
            "total loss: 0.8692145347595215 epoch 5 batch 50 \n",
            "total loss: 1.0533963441848755 epoch 5 batch 55 \n",
            "total loss: 1.0151276588439941 epoch 5 batch 60 \n",
            "total loss: 1.0281364917755127 epoch 5 batch 65 \n",
            "total loss: 0.9657193422317505 epoch 5 batch 70 \n",
            "total loss: 0.9862409830093384 epoch 5 batch 75 \n",
            "total loss: 0.8985677361488342 epoch 5 batch 80 \n",
            "total loss: 0.9518996477127075 epoch 5 batch 85 \n",
            "total loss: 0.9929949045181274 epoch 5 batch 90 \n",
            "total loss: 0.9326523542404175 epoch 5 batch 95 \n",
            "total loss: 0.9437942504882812 epoch 5 batch 100 \n",
            "total loss: 1.0243723392486572 epoch 5 batch 105 \n",
            "total loss: 1.098410964012146 epoch 5 batch 110 \n",
            "total loss: 1.0311801433563232 epoch 5 batch 115 \n",
            "total loss: 0.9585906267166138 epoch 5 batch 120 \n",
            "total loss: 1.0250282287597656 epoch 5 batch 125 \n",
            "total loss: 1.1848608255386353 epoch 5 batch 130 \n",
            "total loss: 1.0160646438598633 epoch 5 batch 135 \n",
            "total loss: 1.0192322731018066 epoch 6 batch 5 \n",
            "total loss: 0.8672206401824951 epoch 6 batch 10 \n",
            "total loss: 0.8857726454734802 epoch 6 batch 15 \n",
            "total loss: 0.8599262237548828 epoch 6 batch 20 \n",
            "total loss: 0.9550821781158447 epoch 6 batch 25 \n",
            "total loss: 0.9291149973869324 epoch 6 batch 30 \n",
            "total loss: 1.020555019378662 epoch 6 batch 35 \n",
            "total loss: 0.8675915002822876 epoch 6 batch 40 \n",
            "total loss: 0.9229665398597717 epoch 6 batch 45 \n",
            "total loss: 0.9764881730079651 epoch 6 batch 50 \n",
            "total loss: 0.9194167256355286 epoch 6 batch 55 \n",
            "total loss: 1.0098682641983032 epoch 6 batch 60 \n",
            "total loss: 0.9145447611808777 epoch 6 batch 65 \n",
            "total loss: 1.0327785015106201 epoch 6 batch 70 \n",
            "total loss: 0.8316162824630737 epoch 6 batch 75 \n",
            "total loss: 0.9033227562904358 epoch 6 batch 80 \n",
            "total loss: 0.9181014895439148 epoch 6 batch 85 \n",
            "total loss: 0.9389197826385498 epoch 6 batch 90 \n",
            "total loss: 0.9539174437522888 epoch 6 batch 95 \n",
            "total loss: 0.9944683313369751 epoch 6 batch 100 \n",
            "total loss: 1.0038374662399292 epoch 6 batch 105 \n",
            "total loss: 1.017403244972229 epoch 6 batch 110 \n",
            "total loss: 1.1035362482070923 epoch 6 batch 115 \n",
            "total loss: 0.9791356921195984 epoch 6 batch 120 \n",
            "total loss: 0.8629512190818787 epoch 6 batch 125 \n",
            "total loss: 0.9070027470588684 epoch 6 batch 130 \n",
            "total loss: 1.0587886571884155 epoch 6 batch 135 \n",
            "total loss: 0.8295857906341553 epoch 7 batch 5 \n",
            "total loss: 0.8694903254508972 epoch 7 batch 10 \n",
            "total loss: 0.9205375909805298 epoch 7 batch 15 \n",
            "total loss: 1.0372689962387085 epoch 7 batch 20 \n",
            "total loss: 0.8783015012741089 epoch 7 batch 25 \n",
            "total loss: 0.8159589767456055 epoch 7 batch 30 \n",
            "total loss: 0.8221981525421143 epoch 7 batch 35 \n",
            "total loss: 0.961735188961029 epoch 7 batch 40 \n",
            "total loss: 0.8390375971794128 epoch 7 batch 45 \n",
            "total loss: 0.864874005317688 epoch 7 batch 50 \n",
            "total loss: 0.9054192900657654 epoch 7 batch 55 \n",
            "total loss: 0.8917567133903503 epoch 7 batch 60 \n",
            "total loss: 0.7558819651603699 epoch 7 batch 65 \n",
            "total loss: 0.7921680212020874 epoch 7 batch 70 \n",
            "total loss: 0.892903208732605 epoch 7 batch 75 \n",
            "total loss: 0.8591558337211609 epoch 7 batch 80 \n",
            "total loss: 0.9252970218658447 epoch 7 batch 85 \n",
            "total loss: 0.8257760405540466 epoch 7 batch 90 \n",
            "total loss: 0.9053861498832703 epoch 7 batch 95 \n",
            "total loss: 0.9087815284729004 epoch 7 batch 100 \n",
            "total loss: 0.8845706582069397 epoch 7 batch 105 \n",
            "total loss: 0.9001150727272034 epoch 7 batch 110 \n",
            "total loss: 0.9319825768470764 epoch 7 batch 115 \n",
            "total loss: 1.0183528661727905 epoch 7 batch 120 \n",
            "total loss: 1.0058118104934692 epoch 7 batch 125 \n",
            "total loss: 0.9291919469833374 epoch 7 batch 130 \n",
            "total loss: 0.8778026700019836 epoch 7 batch 135 \n",
            "total loss: 0.7550419569015503 epoch 8 batch 5 \n",
            "total loss: 0.7828114628791809 epoch 8 batch 10 \n",
            "total loss: 0.7788205146789551 epoch 8 batch 15 \n",
            "total loss: 0.7363832592964172 epoch 8 batch 20 \n",
            "total loss: 0.6708140969276428 epoch 8 batch 25 \n",
            "total loss: 0.7578430771827698 epoch 8 batch 30 \n",
            "total loss: 0.9239441752433777 epoch 8 batch 35 \n",
            "total loss: 0.9034091234207153 epoch 8 batch 40 \n",
            "total loss: 0.8789718747138977 epoch 8 batch 45 \n",
            "total loss: 0.7476455569267273 epoch 8 batch 50 \n",
            "total loss: 0.782870352268219 epoch 8 batch 55 \n",
            "total loss: 0.7766546607017517 epoch 8 batch 60 \n",
            "total loss: 0.7744563221931458 epoch 8 batch 65 \n",
            "total loss: 0.7760935425758362 epoch 8 batch 70 \n",
            "total loss: 0.8319960832595825 epoch 8 batch 75 \n",
            "total loss: 0.7900771498680115 epoch 8 batch 80 \n",
            "total loss: 0.7961241006851196 epoch 8 batch 85 \n",
            "total loss: 0.9427276849746704 epoch 8 batch 90 \n",
            "total loss: 0.809019923210144 epoch 8 batch 95 \n",
            "total loss: 0.808312714099884 epoch 8 batch 100 \n",
            "total loss: 0.8430421948432922 epoch 8 batch 105 \n",
            "total loss: 0.8577835559844971 epoch 8 batch 110 \n",
            "total loss: 0.9481770396232605 epoch 8 batch 115 \n",
            "total loss: 0.9204382300376892 epoch 8 batch 120 \n",
            "total loss: 0.8566911816596985 epoch 8 batch 125 \n",
            "total loss: 0.8925788402557373 epoch 8 batch 130 \n",
            "total loss: 0.8297337889671326 epoch 8 batch 135 \n",
            "total loss: 0.804693877696991 epoch 9 batch 5 \n",
            "total loss: 0.7440676689147949 epoch 9 batch 10 \n",
            "total loss: 0.6850144863128662 epoch 9 batch 15 \n",
            "total loss: 0.7424946427345276 epoch 9 batch 20 \n",
            "total loss: 0.7875263094902039 epoch 9 batch 25 \n",
            "total loss: 0.6824126243591309 epoch 9 batch 30 \n",
            "total loss: 0.6659815907478333 epoch 9 batch 35 \n",
            "total loss: 0.7188621759414673 epoch 9 batch 40 \n",
            "total loss: 0.7236735820770264 epoch 9 batch 45 \n",
            "total loss: 0.7293100953102112 epoch 9 batch 50 \n",
            "total loss: 0.7811743021011353 epoch 9 batch 55 \n",
            "total loss: 0.7320955395698547 epoch 9 batch 60 \n",
            "total loss: 0.7964556813240051 epoch 9 batch 65 \n",
            "total loss: 0.8299012780189514 epoch 9 batch 70 \n",
            "total loss: 0.7798922657966614 epoch 9 batch 75 \n",
            "total loss: 0.8454175591468811 epoch 9 batch 80 \n",
            "total loss: 0.8420031070709229 epoch 9 batch 85 \n",
            "total loss: 0.721431314945221 epoch 9 batch 90 \n",
            "total loss: 0.8177716732025146 epoch 9 batch 95 \n",
            "total loss: 0.7976412177085876 epoch 9 batch 100 \n",
            "total loss: 0.9195285439491272 epoch 9 batch 105 \n",
            "total loss: 0.7694283723831177 epoch 9 batch 110 \n",
            "total loss: 0.9156899452209473 epoch 9 batch 115 \n",
            "total loss: 0.7859970927238464 epoch 9 batch 120 \n",
            "total loss: 0.8687516450881958 epoch 9 batch 125 \n",
            "total loss: 0.8054805397987366 epoch 9 batch 130 \n",
            "total loss: 0.8484460115432739 epoch 9 batch 135 \n",
            "total loss: 0.6770702600479126 epoch 10 batch 5 \n",
            "total loss: 0.5843807458877563 epoch 10 batch 10 \n",
            "total loss: 0.6756654977798462 epoch 10 batch 15 \n",
            "total loss: 0.7295694947242737 epoch 10 batch 20 \n",
            "total loss: 0.7060496211051941 epoch 10 batch 25 \n",
            "total loss: 0.753251314163208 epoch 10 batch 30 \n",
            "total loss: 0.7630420327186584 epoch 10 batch 35 \n",
            "total loss: 0.7650782465934753 epoch 10 batch 40 \n",
            "total loss: 0.6761084198951721 epoch 10 batch 45 \n",
            "total loss: 0.75175940990448 epoch 10 batch 50 \n",
            "total loss: 0.688420295715332 epoch 10 batch 55 \n",
            "total loss: 0.6684751510620117 epoch 10 batch 60 \n",
            "total loss: 0.7259660959243774 epoch 10 batch 65 \n",
            "total loss: 0.7809147238731384 epoch 10 batch 70 \n",
            "total loss: 0.7294312715530396 epoch 10 batch 75 \n",
            "total loss: 0.7425450086593628 epoch 10 batch 80 \n",
            "total loss: 0.7589617967605591 epoch 10 batch 85 \n",
            "total loss: 0.6804299354553223 epoch 10 batch 90 \n",
            "total loss: 0.7469694018363953 epoch 10 batch 95 \n",
            "total loss: 0.7714285850524902 epoch 10 batch 100 \n",
            "total loss: 0.754675567150116 epoch 10 batch 105 \n",
            "total loss: 0.8631336092948914 epoch 10 batch 110 \n",
            "total loss: 0.8069604635238647 epoch 10 batch 115 \n",
            "total loss: 0.8312491178512573 epoch 10 batch 120 \n",
            "total loss: 0.7407167553901672 epoch 10 batch 125 \n",
            "total loss: 0.7932342290878296 epoch 10 batch 130 \n",
            "total loss: 0.8136574625968933 epoch 10 batch 135 \n",
            "total loss: 0.646446168422699 epoch 11 batch 5 \n",
            "total loss: 0.7299992442131042 epoch 11 batch 10 \n",
            "total loss: 0.6875669956207275 epoch 11 batch 15 \n",
            "total loss: 0.6804881691932678 epoch 11 batch 20 \n",
            "total loss: 0.613861083984375 epoch 11 batch 25 \n",
            "total loss: 0.6901577711105347 epoch 11 batch 30 \n",
            "total loss: 0.6563993692398071 epoch 11 batch 35 \n",
            "total loss: 0.6499587893486023 epoch 11 batch 40 \n",
            "total loss: 0.6538712978363037 epoch 11 batch 45 \n",
            "total loss: 0.6924850940704346 epoch 11 batch 50 \n",
            "total loss: 0.7332882285118103 epoch 11 batch 55 \n",
            "total loss: 0.6521152257919312 epoch 11 batch 60 \n",
            "total loss: 0.7091915607452393 epoch 11 batch 65 \n",
            "total loss: 0.7440416812896729 epoch 11 batch 70 \n",
            "total loss: 0.6351345181465149 epoch 11 batch 75 \n",
            "total loss: 0.7074438333511353 epoch 11 batch 80 \n",
            "total loss: 0.7750745415687561 epoch 11 batch 85 \n",
            "total loss: 0.655357301235199 epoch 11 batch 90 \n",
            "total loss: 0.6287118792533875 epoch 11 batch 95 \n",
            "total loss: 0.6810784935951233 epoch 11 batch 100 \n",
            "total loss: 0.6748161911964417 epoch 11 batch 105 \n",
            "total loss: 0.6826025247573853 epoch 11 batch 110 \n",
            "total loss: 0.6144744753837585 epoch 11 batch 115 \n",
            "total loss: 0.6343405842781067 epoch 11 batch 120 \n",
            "total loss: 0.701090931892395 epoch 11 batch 125 \n",
            "total loss: 0.7331063151359558 epoch 11 batch 130 \n",
            "total loss: 0.7453281879425049 epoch 11 batch 135 \n",
            "total loss: 0.5919060707092285 epoch 12 batch 5 \n",
            "total loss: 0.5848358869552612 epoch 12 batch 10 \n",
            "total loss: 0.5778202414512634 epoch 12 batch 15 \n",
            "total loss: 0.6557623744010925 epoch 12 batch 20 \n",
            "total loss: 0.6183678507804871 epoch 12 batch 25 \n",
            "total loss: 0.608422040939331 epoch 12 batch 30 \n",
            "total loss: 0.64106285572052 epoch 12 batch 35 \n",
            "total loss: 0.590716540813446 epoch 12 batch 40 \n",
            "total loss: 0.5811307430267334 epoch 12 batch 45 \n",
            "total loss: 0.7085281610488892 epoch 12 batch 50 \n",
            "total loss: 0.694469690322876 epoch 12 batch 55 \n",
            "total loss: 0.6257287263870239 epoch 12 batch 60 \n",
            "total loss: 0.5969177484512329 epoch 12 batch 65 \n",
            "total loss: 0.6745822429656982 epoch 12 batch 70 \n",
            "total loss: 0.6133835315704346 epoch 12 batch 75 \n",
            "total loss: 0.6022340655326843 epoch 12 batch 80 \n",
            "total loss: 0.6632403135299683 epoch 12 batch 85 \n",
            "total loss: 0.617990255355835 epoch 12 batch 90 \n",
            "total loss: 0.6703490018844604 epoch 12 batch 95 \n",
            "total loss: 0.6486249566078186 epoch 12 batch 100 \n",
            "total loss: 0.6018936038017273 epoch 12 batch 105 \n",
            "total loss: 0.6365768313407898 epoch 12 batch 110 \n",
            "total loss: 0.7150453329086304 epoch 12 batch 115 \n",
            "total loss: 0.6796818375587463 epoch 12 batch 120 \n",
            "total loss: 0.7213588356971741 epoch 12 batch 125 \n",
            "total loss: 0.6467656493186951 epoch 12 batch 130 \n",
            "total loss: 0.731880784034729 epoch 12 batch 135 \n",
            "total loss: 0.5577025413513184 epoch 13 batch 5 \n",
            "total loss: 0.5639230012893677 epoch 13 batch 10 \n",
            "total loss: 0.545904278755188 epoch 13 batch 15 \n",
            "total loss: 0.5051172971725464 epoch 13 batch 20 \n",
            "total loss: 0.5688350200653076 epoch 13 batch 25 \n",
            "total loss: 0.5856376886367798 epoch 13 batch 30 \n",
            "total loss: 0.4648995101451874 epoch 13 batch 35 \n",
            "total loss: 0.5445460081100464 epoch 13 batch 40 \n",
            "total loss: 0.5403193831443787 epoch 13 batch 45 \n",
            "total loss: 0.6595250964164734 epoch 13 batch 50 \n",
            "total loss: 0.5937183499336243 epoch 13 batch 55 \n",
            "total loss: 0.6235274076461792 epoch 13 batch 60 \n",
            "total loss: 0.6127834320068359 epoch 13 batch 65 \n",
            "total loss: 0.5621405243873596 epoch 13 batch 70 \n",
            "total loss: 0.5393080711364746 epoch 13 batch 75 \n",
            "total loss: 0.5082228779792786 epoch 13 batch 80 \n",
            "total loss: 0.5660300254821777 epoch 13 batch 85 \n",
            "total loss: 0.6039773225784302 epoch 13 batch 90 \n",
            "total loss: 0.5855810046195984 epoch 13 batch 95 \n",
            "total loss: 0.5680128931999207 epoch 13 batch 100 \n",
            "total loss: 0.6163796782493591 epoch 13 batch 105 \n",
            "total loss: 0.544130802154541 epoch 13 batch 110 \n",
            "total loss: 0.5901878476142883 epoch 13 batch 115 \n",
            "total loss: 0.6269972920417786 epoch 13 batch 120 \n",
            "total loss: 0.6572248935699463 epoch 13 batch 125 \n",
            "total loss: 0.7013998627662659 epoch 13 batch 130 \n",
            "total loss: 0.5947216749191284 epoch 13 batch 135 \n",
            "total loss: 0.5077345967292786 epoch 14 batch 5 \n",
            "total loss: 0.5052902698516846 epoch 14 batch 10 \n",
            "total loss: 0.5213139057159424 epoch 14 batch 15 \n",
            "total loss: 0.5345025062561035 epoch 14 batch 20 \n",
            "total loss: 0.41437777876853943 epoch 14 batch 25 \n",
            "total loss: 0.5278926491737366 epoch 14 batch 30 \n",
            "total loss: 0.5322567820549011 epoch 14 batch 35 \n",
            "total loss: 0.5469369888305664 epoch 14 batch 40 \n",
            "total loss: 0.530822217464447 epoch 14 batch 45 \n",
            "total loss: 0.5746462941169739 epoch 14 batch 50 \n",
            "total loss: 0.5926828384399414 epoch 14 batch 55 \n",
            "total loss: 0.5718095898628235 epoch 14 batch 60 \n",
            "total loss: 0.5551791787147522 epoch 14 batch 65 \n",
            "total loss: 0.6249980926513672 epoch 14 batch 70 \n",
            "total loss: 0.4778152406215668 epoch 14 batch 75 \n",
            "total loss: 0.5358534455299377 epoch 14 batch 80 \n",
            "total loss: 0.5979921817779541 epoch 14 batch 85 \n",
            "total loss: 0.5507349371910095 epoch 14 batch 90 \n",
            "total loss: 0.6100803017616272 epoch 14 batch 95 \n",
            "total loss: 0.5241344571113586 epoch 14 batch 100 \n",
            "total loss: 0.5126403570175171 epoch 14 batch 105 \n",
            "total loss: 0.5772883296012878 epoch 14 batch 110 \n",
            "total loss: 0.5732390880584717 epoch 14 batch 115 \n",
            "total loss: 0.5185940861701965 epoch 14 batch 120 \n",
            "total loss: 0.5512474179267883 epoch 14 batch 125 \n",
            "total loss: 0.5166537761688232 epoch 14 batch 130 \n",
            "total loss: 0.5591790080070496 epoch 14 batch 135 \n",
            "total loss: 0.5007180571556091 epoch 15 batch 5 \n",
            "total loss: 0.479447603225708 epoch 15 batch 10 \n",
            "total loss: 0.4856577515602112 epoch 15 batch 15 \n",
            "total loss: 0.5658054351806641 epoch 15 batch 20 \n",
            "total loss: 0.5228333473205566 epoch 15 batch 25 \n",
            "total loss: 0.44621744751930237 epoch 15 batch 30 \n",
            "total loss: 0.5031988024711609 epoch 15 batch 35 \n",
            "total loss: 0.49409449100494385 epoch 15 batch 40 \n",
            "total loss: 0.4774528741836548 epoch 15 batch 45 \n",
            "total loss: 0.5007137060165405 epoch 15 batch 50 \n",
            "total loss: 0.44164741039276123 epoch 15 batch 55 \n",
            "total loss: 0.49857598543167114 epoch 15 batch 60 \n",
            "total loss: 0.4755468964576721 epoch 15 batch 65 \n",
            "total loss: 0.5051528811454773 epoch 15 batch 70 \n",
            "total loss: 0.4734129309654236 epoch 15 batch 75 \n",
            "total loss: 0.5113857388496399 epoch 15 batch 80 \n",
            "total loss: 0.5087001323699951 epoch 15 batch 85 \n",
            "total loss: 0.52547687292099 epoch 15 batch 90 \n",
            "total loss: 0.5535438060760498 epoch 15 batch 95 \n",
            "total loss: 0.5723105669021606 epoch 15 batch 100 \n",
            "total loss: 0.5569877624511719 epoch 15 batch 105 \n",
            "total loss: 0.5079530477523804 epoch 15 batch 110 \n",
            "total loss: 0.5445125102996826 epoch 15 batch 115 \n",
            "total loss: 0.5414832830429077 epoch 15 batch 120 \n",
            "total loss: 0.5503643155097961 epoch 15 batch 125 \n",
            "total loss: 0.533541738986969 epoch 15 batch 130 \n",
            "total loss: 0.6088541746139526 epoch 15 batch 135 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDyK-EGqbN5r",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IpV4o3dk6L0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cleanDataset(myDataset):\n",
        "  myCleanDataset = []\n",
        "  for ele in myDataset:\n",
        "    sentence = re.sub('<start> ', '', ele)\n",
        "    sentence = re.sub(' <end>', '', sentence)\n",
        "    myCleanDataset.append(sentence)\n",
        "  return myCleanDataset\n",
        "\n",
        "cleanEnValid = cleanDataset(enValid)\n",
        "cleanFrValid = cleanDataset(frValid)\n",
        "cleanEnTrain = cleanDataset(enTrain)\n",
        "cleanFrTrain = cleanDataset(frTrain)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y98sfom7SuGy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translateSentence(input_raw):\n",
        "  #input_raw=\"so too does the idea that accommodating religious differences is dangerous\"\n",
        "\n",
        "  # We have a transcript file containing English-Hindi pairs\n",
        "  # Preprocess X\n",
        "  input_lines = ['<start> '+input_raw+'']\n",
        "  input_sequences = [[en_tokenizer.word_index[w] for w in line.split(' ')] for line in input_lines]\n",
        "  input_sequences = tf.keras.preprocessing.sequence.pad_sequences(input_sequences,\n",
        "                                                                  maxlen=Tx, padding='post')\n",
        "  inp = tf.convert_to_tensor(input_sequences)\n",
        "  inference_batch_size = input_sequences.shape[0]\n",
        "  encoder_initial_cell_state = [tf.zeros((inference_batch_size, rnn_units)),\n",
        "                                tf.zeros((inference_batch_size, rnn_units))]\n",
        "  encoder_emb_inp = encoderNetwork.encoder_embedding(inp)\n",
        "  a, a_tx, c_tx = encoderNetwork.encoder_rnnlayer(encoder_emb_inp,\n",
        "                                                  initial_state =encoder_initial_cell_state)\n",
        "\n",
        "  start_tokens = tf.fill([inference_batch_size],fr_tokenizer.word_index['<start>'])\n",
        "\n",
        "  end_token = fr_tokenizer.word_index['<end>']\n",
        "\n",
        "  greedy_sampler = tfa.seq2seq.GreedyEmbeddingSampler()\n",
        "\n",
        "  decoder_input = tf.expand_dims([fr_tokenizer.word_index['<start>']]* inference_batch_size,1)\n",
        "  decoder_emb_inp = decoderNetwork.decoder_embedding(decoder_input)\n",
        "\n",
        "  decoder_instance = tfa.seq2seq.BasicDecoder(cell = decoderNetwork.rnn_cell, sampler = greedy_sampler,\n",
        "                                              output_layer=decoderNetwork.dense_layer)\n",
        "  decoderNetwork.attention_mechanism.setup_memory(a)\n",
        "  decoder_initial_state = decoderNetwork.build_decoder_initial_state(inference_batch_size,\n",
        "                                                                    encoder_state=[a_tx, c_tx],\n",
        "                                                                    Dtype=tf.float32)\n",
        "\n",
        "  # Since we do not know the target sequence lengths in advance, we use maximum_iterations to limit the translation lengths.\n",
        "  # One heuristic is to decode up to two times the source sentence lengths.\n",
        "  maximum_iterations = tf.round(tf.reduce_max(Tx) * 2)\n",
        "\n",
        "  #initialize inference decoder\n",
        "  decoder_embedding_matrix = decoderNetwork.decoder_embedding.variables[0] \n",
        "  (first_finished, first_inputs,first_state) = decoder_instance.initialize(decoder_embedding_matrix,\n",
        "                              start_tokens = start_tokens,\n",
        "                              end_token=end_token,\n",
        "                              initial_state = decoder_initial_state)\n",
        " \n",
        "  inputs = first_inputs\n",
        "  state = first_state  \n",
        "  predictions = np.empty((inference_batch_size,0), dtype = np.int32)                                                                             \n",
        "  for j in range(maximum_iterations):\n",
        "      outputs, next_state, next_inputs, finished = decoder_instance.step(j,inputs,state)\n",
        "      inputs = next_inputs\n",
        "      state = next_state\n",
        "      outputs = np.expand_dims(outputs.sample_id,axis = -1)\n",
        "      predictions = np.append(predictions, outputs, axis = -1)\n",
        "  \n",
        "  for i in range(len(predictions)):\n",
        "    line = predictions[i,:]\n",
        "    seq = list(itertools.takewhile( lambda index: index !=2, line))\n",
        "    sentence = \"\"\n",
        "    for w in seq:\n",
        "      word = fr_tokenizer.index_word[w]\n",
        "      if word != \"<end>\":\n",
        "        sentence += word\n",
        "        sentence += \" \"\n",
        "      else:\n",
        "        break\n",
        "  return sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iodjSItQds1t",
        "colab_type": "text"
      },
      "source": [
        "## Final Translation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76L09JEIzo3m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "translatedSentences = []\n",
        "trainTranslate = []\n",
        "for sentence in cleanEnTrain[0:100]:\n",
        "  trainTranslate.append(translateSentence(sentence))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khKHv1313dGe",
        "colab_type": "code",
        "outputId": "006fb16c-837d-471d-c2a4-7dd50501acc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "pip install sacrebleu"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sacrebleu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/58/5c6cc352ea6271125325950715cf8b59b77abe5e93cf29f6e60b491a31d9/sacrebleu-1.4.6-py3-none-any.whl (59kB)\n",
            "\r\u001b[K     |█████▌                          | 10kB 25.8MB/s eta 0:00:01\r\u001b[K     |███████████                     | 20kB 3.1MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 30kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 40kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 51kB 3.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 3.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from sacrebleu) (3.6.6)\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/64/03/9abfb3374d67838daf24f1a388528714bec1debb1d13749f0abd7fb07cfb/portalocker-1.6.0-py2.py3-none-any.whl\n",
            "Collecting mecab-python3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/49/b55a839a77189042960bf96490640c44816073f917d489acbc5d79fa5cc3/mecab_python3-0.996.5-cp36-cp36m-manylinux2010_x86_64.whl (17.1MB)\n",
            "\u001b[K     |████████████████████████████████| 17.1MB 203kB/s \n",
            "\u001b[?25hInstalling collected packages: portalocker, mecab-python3, sacrebleu\n",
            "Successfully installed mecab-python3-0.996.5 portalocker-1.6.0 sacrebleu-1.4.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duhSFF52zq7F",
        "colab_type": "code",
        "outputId": "b03a2f0e-2873-4ba4-b64a-a537ab32044b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import sacrebleu\n",
        "\n",
        "bleu_scores = []\n",
        "for i in range(len(trainTranslate)):\n",
        "    bleu_scores += [sacrebleu.corpus_bleu(trainTranslate[i], cleanFrTrain[i]).score]\n",
        "    \n",
        "np.mean(bleu_scores)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['donc , cela ne peut pas etre en energie ', 'monsieur le president , monsieur le commissaire , mesdames et messieurs , je voudrais remercier le rapporteur pour sa premiere cooperation ', 'la commission presentera un role proche de la publicite , mais je souhaite , en principe de compte , une fois de soutien publique et concrete , est un pourcentage d emissions d assistance et de lutte contre le terrorisme ', 'il est imperatif que ce rapport est raisonnable ', 'la commission de l environnement et de la sante publique et de la politique de l union europeenne est que l europe est une faible de la dette ', 'en outre , la commission a publie une proposition visant a mettre en place une politique coherente et constante d aide a l encontre des criteres de lutte utilisee et evaluer la corruption internationale a l egard de la communaute mondiale ', 'mais , monsieur le commissaire , mesdames et messieurs , je voudrais vous remercier de votre attention ', 'ils ont egalise les conditions de craindre qui vivent dans des organismes de police ', 'la commission presentera un role proche de la publicite , mais je souhaite , en principe de compte , une fois de soutien publique et concrete , est un pourcentage d emissions d assistance et de lutte contre le terrorisme ', 'nous avons vu des reponses concretes des editeurs ']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.6747765385227456"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6Av-oPWvRc4",
        "colab_type": "text"
      },
      "source": [
        "### The accuracy can be improved by implementing:\n",
        "* Beam Search or Lexicon Search\n",
        "* Bi-directional encoder-decoder model "
      ]
    }
  ]
}