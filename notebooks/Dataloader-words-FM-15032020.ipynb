{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: François Mercier\n",
    "\n",
    "Goals: \n",
    "- Convert preprocessed into TF dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "from functools import partial\n",
    "\n",
    "from fastprogress import progress_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\") # Require to have the utilities packages in path\n",
    "from tools import tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 999)\n",
    "pd.set_option('display.max_colwidth', 999)\n",
    "pd.set_option('display.max_rows', 999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/project/cq-training-1/project2/teams/team03/data/preprocessed_15032020/token_to_word_en.pickle'),\n",
       " PosixPath('/project/cq-training-1/project2/teams/team03/data/preprocessed_15032020/train_lang1_en_numericalized.pickle'),\n",
       " PosixPath('/project/cq-training-1/project2/teams/team03/data/preprocessed_15032020/word_to_token_fr.pickle'),\n",
       " PosixPath('/project/cq-training-1/project2/teams/team03/data/preprocessed_15032020/unaligned_fr_numericalized.pickle'),\n",
       " PosixPath('/project/cq-training-1/project2/teams/team03/data/preprocessed_15032020/word_to_token_en.pickle'),\n",
       " PosixPath('/project/cq-training-1/project2/teams/team03/data/preprocessed_15032020/train_lang2_fr_numericalized.pickle'),\n",
       " PosixPath('/project/cq-training-1/project2/teams/team03/data/preprocessed_15032020/token_to_word_fr.pickle'),\n",
       " PosixPath('/project/cq-training-1/project2/teams/team03/data/preprocessed_15032020/unaligned_en_numericalized.pickle')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = Path(r\"/project/cq-training-1/project2/teams/team03/data/preprocessed_15032020\")\n",
    "files = list(data_path.glob(\"*\"))\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bilingual dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path/\"train_lang1_en_numericalized.pickle\", 'rb') as handle:\n",
    "    train_lang1_en_numericalized = pickle.load(handle)\n",
    "    \n",
    "with open(data_path/\"train_lang2_fr_numericalized.pickle\", 'rb') as handle:\n",
    "    train_lang2_fr_numericalized = pickle.load(handle)\n",
    "    \n",
    "with open(data_path/\"word_to_token_en.pickle\", 'rb') as handle:\n",
    "    word_to_token_en = pickle.load(handle)\n",
    "    \n",
    "with open(data_path/\"word_to_token_fr.pickle\", 'rb') as handle:\n",
    "    word_to_token_fr = pickle.load(handle)\n",
    "\n",
    "with open(data_path/\"token_to_word_fr.pickle\", 'rb') as handle:\n",
    "    token_to_word_fr = pickle.load(handle)\n",
    "\n",
    "with open(data_path/\"token_to_word_en.pickle\", 'rb') as handle:\n",
    "    token_to_word_en = pickle.load(handle)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_ds = zip(train_lang1_en_numericalized, train_lang2_fr_numericalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_generator(train_lang1_en_numericalized=train_lang1_en_numericalized, \n",
    "                 train_lang2_fr_numericalized=train_lang2_fr_numericalized,\n",
    "                ):\n",
    "    bos, eos = -2, -1\n",
    "    for i in range(len(train_lang1_en_numericalized)):\n",
    "        en = np.array([bos] + train_lang1_en_numericalized[i] + [eos]) + 3\n",
    "        fr = np.array([bos] + train_lang2_fr_numericalized[i] + [eos]) + 3\n",
    "        inputs = (en, \n",
    "                  fr)\n",
    "        output = fr[1:]\n",
    "        yield (inputs, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16 # For 2 K80\n",
    "valid_size = 1000\n",
    "\n",
    "ds = tf.data.Dataset.from_generator(my_generator, \n",
    "                                    output_types=((tf.int32, tf.int32), tf.int32), \n",
    "                                    output_shapes=((tf.TensorShape([None]), tf.TensorShape([None])), \n",
    "                                                   tf.TensorShape([None])))\n",
    "ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "ds = ds.shuffle(seed=42, buffer_size=256)\n",
    "#ds = ds.map(lambda x, y: ((tf.minimum(x[0], 10000 - 1), tf.minimum([1], 10000 - 1)), tf.minimum(y, 10000 - 1))) # Only to test performance with lower vocab size (and GPU mem)\n",
    "ds = ds.padded_batch(batch_size=batch_size, padded_shapes=(([128], [128]), 128))\n",
    "\n",
    "# 5000 like XNLI https://www.nyu.edu/projects/bowman/xnli/\n",
    "test_dataset = ds.take(int(valid_size / batch_size))\n",
    "train_dataset = ds.skip(int(valid_size / batch_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 128) (16, 128) (16, 128)\n",
      "CPU times: user 67.4 ms, sys: 16.9 ms, total: 84.2 ms\n",
      "Wall time: 166 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for element in test_dataset.take(1): \n",
    "    print(element[0][0].shape, element[0][1].shape, element[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91269"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_to_token_fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monolingual dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path/\"unaligned_fr_numericalized.pickle\", 'rb') as handle:\n",
    "    unaligned_fr_numericalized = pickle.load(handle)\n",
    "    \n",
    "with open(data_path/\"unaligned_en_numericalized.pickle\", 'rb') as handle:\n",
    "    unaligned_en_numericalized = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_generator_monolingual(monolingual_numericalized):\n",
    "    bos, eos = -2, -1\n",
    "    for i in range(len(train_lang1_en_numericalized)):\n",
    "        inputs = np.array([bos] + monolingual_numericalized[i] + [eos]) + 3\n",
    "        output = inputs[1:]\n",
    "        yield (inputs, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_monolingual = 16 # For 2 K80\n",
    "valid_size_monolingual = 1000\n",
    "\n",
    "ds_monolingual_fr = tf.data.Dataset.from_generator(partial(my_generator_monolingual, monolingual_numericalized=unaligned_fr_numericalized), \n",
    "                                    output_types=(tf.int32, tf.int32), \n",
    "                                    output_shapes=(tf.TensorShape([None]), \n",
    "                                                   tf.TensorShape([None])))\n",
    "ds_monolingual_fr = ds_monolingual_fr.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "ds_monolingual_fr = ds_monolingual_fr.shuffle(seed=42, buffer_size=256)\n",
    "#ds = ds_monolingual_fr.map(lambda x, y: ((tf.minimum(x[0], 10000 - 1), tf.minimum([1], 10000 - 1)), tf.minimum(y, 10000 - 1))) # Only to test performance with lower vocab size (and GPU mem)\n",
    "ds_monolingual_fr = ds_monolingual_fr.padded_batch(batch_size=batch_size_monolingual, padded_shapes=([128], 128))\n",
    "\n",
    "# 5000 like XNLI https://www.nyu.edu/projects/bowman/xnli/\n",
    "test_dataset_monolingual_fr = ds_monolingual_fr.take(int(valid_size_monolingual / batch_size))#.cache()\n",
    "train_dataset_monolingual_fr = ds_monolingual_fr.skip(int(valid_size_monolingual / batch_size))#.cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 128) (16, 128)\n",
      "CPU times: user 183 ms, sys: 13 ms, total: 196 ms\n",
      "Wall time: 197 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for element in train_dataset_monolingual_fr.take(1): \n",
    "    print(element[0].shape, element[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq at word level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   [(None, 128)]             0         \n",
      "_________________________________________________________________\n",
      "encoder_embedding (Embedding (None, 128, 200)          12091400  \n",
      "_________________________________________________________________\n",
      "encoder_lstm (LSTM)          [(None, 128, 256), (None, 467968    \n",
      "_________________________________________________________________\n",
      "encoder_time_distributed (Ti (None, 128, 60457)        15537449  \n",
      "=================================================================\n",
      "Total params: 28,096,817\n",
      "Trainable params: 28,096,817\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"Decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoder_input (InputLayer)   [(None, 128)]             0         \n",
      "_________________________________________________________________\n",
      "decoder_embedding (Embedding (None, 128, 200)          18254400  \n",
      "_________________________________________________________________\n",
      "decoder_lstm (LSTM)          [(None, 128, 256), (None, 467968    \n",
      "_________________________________________________________________\n",
      "decoder_time_distributed (Ti (None, 128, 91272)        23456904  \n",
      "=================================================================\n",
      "Total params: 42,179,272\n",
      "Trainable params: 42,179,272\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"Full_Model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input (InputLayer)      [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_embedding (Embedding)   (None, 128, 200)     12091400    encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "decoder_embedding (Embedding)   (None, 128, 200)     18254400    decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "encoder_lstm (LSTM)             [(None, 128, 256), ( 467968      encoder_embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm (LSTM)             [(None, 128, 256), ( 467968      decoder_embedding[0][0]          \n",
      "                                                                 encoder_lstm[0][1]               \n",
      "                                                                 encoder_lstm[0][2]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_time_distributed (TimeD (None, 128, 91272)   23456904    decoder_lstm[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 54,738,640\n",
      "Trainable params: 54,738,640\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# hparams\n",
    "latent_dim = 256\n",
    "embedding_dim = 200\n",
    "\n",
    "max_len = 128\n",
    "\n",
    "vocab_size_en = len(word_to_token_en) + 3\n",
    "vocab_size_fr = len(word_to_token_fr) + 3\n",
    "#vocab_size_en = 10000\n",
    "#vocab_size_fr = 10000\n",
    "\n",
    "\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = tf.keras.layers.Input(shape=(max_len), name=\"encoder_input\")\n",
    "encoder_embeddings = tf.keras.layers.Embedding(vocab_size_en, embedding_dim, mask_zero=True, name=\"encoder_embedding\")\n",
    "encoder_lstm = tf.keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True, name=\"encoder_lstm\")\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embeddings(encoder_inputs))\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "encoder_dense = tf.keras.layers.Dense(vocab_size_en, activation='softmax', name=\"encoder_dense\")\n",
    "encoder_time_distributed = tf.keras.layers.TimeDistributed(encoder_dense, name=\"encoder_time_distributed\")\n",
    "encoder_outputs = encoder_time_distributed(encoder_outputs)\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = tf.keras.layers.Input(shape=(max_len), name=\"decoder_input\")\n",
    "\n",
    "decoder_embeddings = tf.keras.layers.Embedding(vocab_size_fr, embedding_dim, mask_zero=True, name=\"decoder_embedding\")\n",
    "decoder_lstm = tf.keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True, name=\"decoder_lstm\")\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embeddings(decoder_inputs),\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = tf.keras.layers.Dense(vocab_size_fr, activation='softmax', name=\"decoder_dense\")\n",
    "decoder_time_distributed = tf.keras.layers.TimeDistributed(decoder_dense, name=\"decoder_time_distributed\")\n",
    "decoder_outputs = decoder_time_distributed(decoder_outputs)\n",
    "\n",
    "\n",
    "decoder_state_input_h = tf.keras.layers.Input(shape=(latent_dim,), name=\"decoder_input_h\")\n",
    "decoder_state_input_c = tf.keras.layers.Input(shape=(latent_dim,), name=\"decoder_input_c\")\n",
    "\n",
    "# Decoder for inference (no states from encoder)\n",
    "decoder_outputs_inference, _, _ = decoder_lstm(decoder_embeddings(decoder_inputs))\n",
    "decoder_outputs_inference = decoder_time_distributed(decoder_outputs_inference)\n",
    "\n",
    "# Multi GPU settings\n",
    "nb_gpus = len(tf.config.experimental.list_physical_devices('GPU'))\n",
    "mirrored_strategy = tf.distribute.MirroredStrategy([\"/gpu:\" + str(i) for i in range(min(2, nb_gpus))])\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "if mirrored_strategy is not None and mirrored_strategy.num_replicas_in_sync > 1:\n",
    "    with mirrored_strategy.scope():\n",
    "        model_monolingual_fr = tf.keras.Model(encoder_inputs, encoder_outputs, name=\"Encoder\")\n",
    "        model_monolingual_en = tf.keras.Model(decoder_inputs, decoder_outputs_inference, name=\"Decoder\")\n",
    "        model = tf.keras.Model([encoder_inputs, decoder_inputs], decoder_outputs, name=\"Full_Model\")\n",
    "else:\n",
    "    model_monolingual_fr = tf.keras.Model(encoder_inputs, encoder_outputs, name=\"Encoder\")\n",
    "    model_monolingual_en = tf.keras.Model(decoder_inputs, decoder_outputs_inference, name=\"Decoder\")\n",
    "    model = tf.keras.Model([encoder_inputs, decoder_inputs], decoder_outputs, name=\"Full_Model\")\n",
    "\n",
    "model_monolingual_fr.summary()\n",
    "model_monolingual_en.summary()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_not_pretrained = tf.keras.models.clone_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "30/30 [==============================] - 73s 2s/step - loss: 1.7049 - accuracy: 0.0906 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "30/30 [==============================] - 51s 2s/step - loss: 1.2214 - accuracy: 0.1879 - val_loss: 1.2269 - val_accuracy: 0.2012\n",
      "Epoch 3/10\n",
      "30/30 [==============================] - 51s 2s/step - loss: 1.1169 - accuracy: 0.2108 - val_loss: 1.1781 - val_accuracy: 0.2124\n",
      "Epoch 4/10\n",
      "30/30 [==============================] - 52s 2s/step - loss: 1.0207 - accuracy: 0.2364 - val_loss: 1.1386 - val_accuracy: 0.2318\n",
      "Epoch 5/10\n",
      "30/30 [==============================] - 52s 2s/step - loss: 0.9232 - accuracy: 0.2676 - val_loss: 1.1137 - val_accuracy: 0.2399\n",
      "Epoch 6/10\n",
      "30/30 [==============================] - 50s 2s/step - loss: 0.8707 - accuracy: 0.2918 - val_loss: 1.0957 - val_accuracy: 0.2498\n",
      "Epoch 7/10\n",
      "30/30 [==============================] - 51s 2s/step - loss: 0.8038 - accuracy: 0.3158 - val_loss: 1.0813 - val_accuracy: 0.2557\n",
      "Epoch 8/10\n",
      "30/30 [==============================] - 52s 2s/step - loss: 0.7574 - accuracy: 0.3393 - val_loss: 1.0815 - val_accuracy: 0.2618\n",
      "Epoch 9/10\n",
      "30/30 [==============================] - 51s 2s/step - loss: 0.6762 - accuracy: 0.3677 - val_loss: 1.0746 - val_accuracy: 0.2615\n",
      "Epoch 10/10\n",
      "30/30 [==============================] - 52s 2s/step - loss: 0.6441 - accuracy: 0.4090 - val_loss: 1.0738 - val_accuracy: 0.2519\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2ab15466e450>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run training\n",
    "model_not_pretrained.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-2), loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model_not_pretrained.fit(train_dataset.take(30), \n",
    "                         validation_data=test_dataset,\n",
    "                         validation_steps=int(valid_size / batch_size), \n",
    "                         callbacks=[callback], \n",
    "                         epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "300/300 [==============================] - 185s 617ms/step - loss: 1.0712 - accuracy: 0.1850 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "300/300 [==============================] - 173s 576ms/step - loss: 0.9171 - accuracy: 0.2365 - val_loss: 0.9639 - val_accuracy: 0.2310\n",
      "Epoch 3/10\n",
      "300/300 [==============================] - 173s 577ms/step - loss: 0.8515 - accuracy: 0.2658 - val_loss: 0.9533 - val_accuracy: 0.2367\n",
      "Epoch 4/10\n",
      "300/300 [==============================] - 173s 575ms/step - loss: 0.8075 - accuracy: 0.2899 - val_loss: 0.9643 - val_accuracy: 0.2408\n",
      "Epoch 5/10\n",
      "300/300 [==============================] - 173s 577ms/step - loss: 0.7707 - accuracy: 0.3123 - val_loss: 0.9665 - val_accuracy: 0.2407\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2ab16f5fd950>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run languague model training for encoder\n",
    "model_monolingual_fr.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-2), loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model_monolingual_fr.fit(train_dataset_monolingual_fr.take(300), \n",
    "                         validation_data=test_dataset_monolingual_fr, \n",
    "                         validation_steps=int(valid_size_monolingual / batch_size_monolingual), \n",
    "                         callbacks=[callback], \n",
    "                         epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "30/30 [==============================] - 73s 2s/step - loss: 1.4278 - accuracy: 0.1033 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "30/30 [==============================] - 51s 2s/step - loss: 1.1509 - accuracy: 0.1817 - val_loss: 1.2005 - val_accuracy: 0.1932\n",
      "Epoch 3/10\n",
      "30/30 [==============================] - 51s 2s/step - loss: 1.0661 - accuracy: 0.2082 - val_loss: 1.1560 - val_accuracy: 0.2156\n",
      "Epoch 4/10\n",
      "30/30 [==============================] - 51s 2s/step - loss: 0.9868 - accuracy: 0.2406 - val_loss: 1.1060 - val_accuracy: 0.2340\n",
      "Epoch 5/10\n",
      "30/30 [==============================] - 51s 2s/step - loss: 0.8853 - accuracy: 0.2725 - val_loss: 1.1073 - val_accuracy: 0.2362\n",
      "Epoch 6/10\n",
      "30/30 [==============================] - 51s 2s/step - loss: 0.7978 - accuracy: 0.3097 - val_loss: 1.0709 - val_accuracy: 0.2523\n",
      "Epoch 7/10\n",
      "30/30 [==============================] - 51s 2s/step - loss: 0.7476 - accuracy: 0.3476 - val_loss: 1.0637 - val_accuracy: 0.2587\n",
      "Epoch 8/10\n",
      "30/30 [==============================] - 51s 2s/step - loss: 0.6711 - accuracy: 0.3847 - val_loss: 1.0767 - val_accuracy: 0.2647\n",
      "Epoch 9/10\n",
      "30/30 [==============================] - 51s 2s/step - loss: 0.6348 - accuracy: 0.4280 - val_loss: 1.0583 - val_accuracy: 0.2718\n",
      "Epoch 10/10\n",
      "30/30 [==============================] - 51s 2s/step - loss: 0.5755 - accuracy: 0.4790 - val_loss: 1.0497 - val_accuracy: 0.2751\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2ab17ad23950>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run training\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-2), loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(train_dataset.take(30), \n",
    "          validation_data=test_dataset, \n",
    "          validation_steps=int(valid_size / batch_size), \n",
    "          callbacks=[callback], \n",
    "          epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLEU score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For an unknown reason, for the predict, inputs must be float (unlike train)\n",
    "x, y = next(test_dataset.take(1).__iter__())\n",
    "preds = model.predict(x)\n",
    "\n",
    "token_to_word_en_with_special_tokens = {(k+3): v for k, v in token_to_word_en.items()}\n",
    "token_to_word_en_with_special_tokens[0] = \"<MASK>\"\n",
    "token_to_word_en_with_special_tokens[1] = \"<BOS>\"\n",
    "token_to_word_en_with_special_tokens[2] = \"<EOS>\"\n",
    "\n",
    "\n",
    "english = []\n",
    "for sen in x[0]:\n",
    "    trunc_sen = []\n",
    "    for i in sen.numpy():\n",
    "        if i in (0, 1): # MASKING or BOS\n",
    "            continue\n",
    "        if i == 2: # EOS\n",
    "            break\n",
    "        trunc_sen += [token_to_word_en_with_special_tokens[i]]\n",
    "    english += [\" \".join(trunc_sen)]\n",
    "    \n",
    "\n",
    "token_to_word_fr_with_special_tokens = {(k+3): v for k, v in token_to_word_fr.items()}\n",
    "token_to_word_fr_with_special_tokens[0] = \"<MASK>\"\n",
    "token_to_word_fr_with_special_tokens[1] = \"<BOS>\"\n",
    "token_to_word_fr_with_special_tokens[2] = \"<EOS>\"\n",
    "\n",
    "\n",
    "refs = []\n",
    "for sen in y:\n",
    "    trunc_sen = []\n",
    "    for i in sen.numpy():\n",
    "        if i in (0, 1): # MASKING or BOS\n",
    "            continue\n",
    "        if i == 2: # EOS\n",
    "            break\n",
    "        trunc_sen += [token_to_word_fr_with_special_tokens[i]]\n",
    "    refs += [\" \".join(trunc_sen)]\n",
    "\n",
    "\n",
    "sys = []\n",
    "for sen in preds:\n",
    "    trunc_sen = []\n",
    "    for t in sen:\n",
    "        i = t.argmax()\n",
    "        if i in (0, 1): # MASKING or BOS\n",
    "            continue\n",
    "        if i == 2: # EOS\n",
    "            break\n",
    "        trunc_sen += [token_to_word_fr_with_special_tokens[i]]\n",
    "    sys += [\" \".join(trunc_sen)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('that is why we ask the house to support our amendment earmarking eur 500 million for iraq',\n",
       " 'C ’ est pourquoi nous demandons à l ’ Assemblée de soutenir notre amendement prévoyant de réserver 500 millions d ’ euros au profit de l ’ Irak .',\n",
       " \"Le ' est pourquoi je avons que la ' amendement , la le politique et , la , ou de ' une par niveau de la ' Union .\")"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = np.random.randint(low=0, high=len(preds), size=1)[0]\n",
    "english[i], refs[i], sys[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.853537378315519"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sacrebleu\n",
    "\n",
    "bleu_scores = []\n",
    "for i in range(len(sys)):\n",
    "    bleu_scores += [sacrebleu.corpus_bleu(sys[i], refs[i]).score]\n",
    "    \n",
    "np.mean(bleu_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_tf",
   "language": "python",
   "name": "py37_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
